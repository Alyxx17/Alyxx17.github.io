---
title: 卡尔曼滤波基本原理
date: 2026-02-16 20:00:00 +0800
categories: [控制,算法]
tags: [控制]
math: true  # <-- 就是这一行，启用本页的数学公式渲染
---

## 序  

> That's one small step for man, one giant leap for mankind. ——Neil Alden Armstrong


![](assets/img/study/Aldrin_Apollo_11_original.webp)

1969年7月20日，阿姆斯特朗踏上月球，说出了这句传世之言。

全世界都听到了这句话。但很少有人知道，在他踏出这一步的瞬间，有一双看不见的手，正在默默计算着他的位置。

那不是人的手。

是卡尔曼滤波(Kalman Filter)。  

登月舱从月球轨道下降到地表，整个过程需要实时知道两件事：我在哪，我要去哪。

但测量是不准的——雷达有噪声，惯性测量单元有漂移。如果只信传感器，会偏；如果只信数学模型，也会偏。

卡尔曼滤波做的事，就是把这两个不太靠谱的信息来源捏在一起，捏出一个足够靠谱的答案。

它不是什么神秘的黑魔法。它是一套数学公式，写成了代码，烧进了阿波罗制导计算机的只读内存里。

这是那段代码的部分：  

![](assets/img/study/code.webp)

这段代码写在人类登月前。它用AGC汇编语言写成，运行在只有2K内存的计算机上，以今天的标准看简陋得不可思议。

但它管用。

它把阿姆斯特朗送到了静海，又把他带回了家。



## 预备知识

### 矩阵  

矩阵的基本概念，运算规则等。  

### 期望  

在概率论和统计学中，一个离散性随机变量的期望是试验中每次可能的结果乘以其结果概率的总和。例如，掷一枚公平的六面骰子，其每次“点数”的期望是$3.5$，计算如下：

$$
E(X) = 1 \cdot \frac{1}{6} + 2 \cdot \frac{1}{6} + 3 \cdot \frac{1}{6} + 4 \cdot \frac{1}{6} + 5 \cdot \frac{1}{6} + 6 \cdot \frac{1}{6}
= \frac{1 + 2 + 3 + 4 + 5 + 6}{6} = 3.5
$$

如果$X$是连续的随机变量，存在一个相应的概率密度函数$f(x)$，若积分$\int\limits_{-∞}^{∞} xf(x)dx$绝对收敛，那么$X$的期望可以计算为：  

$$
E(X)=\int\limits_{-∞}^{∞} xf(x)dx
$$

是针对于连续的随机变量的，与离散随机变量的期望的算法同出一辙，由于输出值是连续的，所以把求和改成了积分。

期望$E$是线性函数，即:$E(aX+bY)=aE(X)+bE(Y)$  

### 方差   

方差是刻画随机变量在其中心位置附近散布程度的数学特征，反映了随机变量取值的离散程度。方差提供了衡量数据在平均值附近波动程度的方法：方差越大，数据分布越分散，波动越大；方差越小，数据越集中，波动越小。为了消除偏差带来的正负抵消，方差对偏离均值的差值进行平方后取期望。方差的常用符号有：$σ^2，s^2，Var(X)，D(X)$。方差的计算公式如下：

$$
Var(x)=E[(X-\mu )^2]
$$

其中$\mu=E(X)$。


### 协方差矩阵

在概率论与统计学中，协方差（Covariance）用于衡量随机变量间的相关程度。  

设$X$和$Y$为两个实值随机变量，它们的协方差定义为它们偏离各自期望值的乘积的期望值，即：  

$$
Cov(X, Y) = E[(X - E[X])(Y - E[Y])]
$$

可以看出协方差的形式类似于方差，只是把其中的一个随机变量$X$换成了$Y$。

通过利用期望的线性性质，协方差的计算公式可以简化为：  

$$
\begin{aligned}
Cov(X, Y) &= E[(X - E[X])(Y - E[Y])] \\
&= E[XY - XE[Y] - E[X]Y + E[X]E[Y]] \\
&= E[XY] - E[X]E[Y] - E[X]E[Y] + E[X]E[Y] \\
&= E[XY] - E[X]E[Y]
\end{aligned}
$$

当协方差为正值时，表明随机变量X和Y倾向于同时偏离其平均值，呈正相关关系；反之，若协方差为负值，则表明一个变量高于平均值时，另一个倾向于低于平均值，呈负相关关系。如果协方差为零，这意味着两个变量之间没有线性关系。

若随机变量$X$与$Y$是相互独立的，那么$Cov(X, Y)=0$。反之，如果两个变量的协方差为$0$（即不相关），不能推出它们相互独立。  

下图表明了协方差的意义

![协方差的意义](assets/img/study/Covariance_trends.webp)  

这个可以推广到随机变量是向量的情况。在这种情况下,之前定义的标量变成向量和矩阵。已知一个$n$维的随机变量$X$和一个$m$维的随机变量$Y$(假设$X$和$Y$都是列向量),它们的互相关矩阵定义为：  

$$
R_{XY} = E(XY^T) =
\begin{bmatrix}
E(X_1 Y_1) & \cdots & E(X_1 Y_m) \\
\vdots & \ddots & \vdots \\
E(X_n Y_1) & \cdots & E(X_n Y_m)
\end{bmatrix}
$$
 
协方差矩阵定义为:   

$$
C_{XY} = E[(X - \overline{X})(Y - \overline{Y})^T] = E(XY^T) - \overline{X}\overline{Y}^T
$$

这个$n$维的随机变量的自相关矩阵定义为:  

$$
R_X = E[XX^T] =
\begin{bmatrix}
E[X_1^2] & \cdots & E[X_1 X_n] \\
\vdots & \ddots & \vdots \\
E[X_n X_1] & \cdots & E[X_n^2]
\end{bmatrix}
$$

为了更好的理解协方差矩阵，接下来举一个例子。假设如下数据：  

| 类型   | 概率 $p$ | $X₁$ (学习时长) | $X₂$ (作业完成率) | $Y₁$ (数学) | $Y₂$ (语文) |
| :----- | :----- | :------------ | :-------------- | :-------- | :-------- |
| 学霸   | 0.2    | 12            | 1.0 (100%)      | 95        | 90        |
| 普通生 | 0.5    | 8             | 0.8 (80%)       | 75        | 70        |
| 学困生 | 0.3    | 4             | 0.5 (50%)       | 55        | 50        |

**步骤1：计算每类的 $XY^T$（外积）**

学霸：

$$
XY^T = \begin{bmatrix} 12 \\ 1.0 \end{bmatrix} \begin{bmatrix} 95 & 90 \end{bmatrix} = \begin{bmatrix} 1140 & 1080 \\95 & 90 \end{bmatrix}
$$

普通生：

$$
XY^T = \begin{bmatrix} 8 \\ 0.8 \end{bmatrix} \begin{bmatrix} 75 & 70 \end{bmatrix} = \begin{bmatrix} 600 & 560 \\ 60 & 56 \end{bmatrix}
$$

学困生：

$$
XY^T = \begin{bmatrix} 4 \\ 0.5 \end{bmatrix} \begin{bmatrix} 55 & 50 \end{bmatrix} = \begin{bmatrix} 220 & 200 \\ 27.5 & 25 \end{bmatrix}
$$

**步骤2：计算 $E(XY^T)$（加权平均）**

$$
E(XY^T) = 0.2 \times \begin{bmatrix} 1140 & 1080 \\ 95 & 90 \end{bmatrix} + 0.5 \times \begin{bmatrix} 600 & 560 \\ 60 & 56 \end{bmatrix} + 0.3 \times \begin{bmatrix} 220 & 200 \\ 27.5 & 25 \end{bmatrix}
$$

逐元素计算：

$E(X_1 Y_1) = 0.2 \times 1140 + 0.5 \times 600 + 0.3 \times 220 = 594$

$E(X_1 Y_2) = 0.2 \times 1080 + 0.5 \times 560 + 0.3 \times 200 = 556$

$E(X_2 Y_1) = 0.2 \times 95 + 0.5 \times 60 + 0.3 \times 27.5 = 57.25$

$E(X_2 Y_2) = 0.2 \times 90 + 0.5 \times 56 + 0.3 \times 25 = 53.5$

所以：

$$
R_{XY} = E(XY^T) = \begin{bmatrix} 594 & 556 \\ 57.25 & 53.5 \end{bmatrix}
$$

**步骤3：计算各自的期望 $E(X)$ 和 $E(Y)$** 

$$
E(X) = \begin{bmatrix} E(X_1) \\ E(X_2) \end{bmatrix} = \begin{bmatrix} 0.2 \times 12 + 0.5 \times 8 + 0.3 \times 4 \\ 0.2 \times 1.0 + 0.5 \times 0.8 + 0.3 \times 0.5 \end{bmatrix} = \begin{bmatrix} 7.6 \\ 0.75 \end{bmatrix}
$$

$$
E(Y) = \begin{bmatrix} E(Y_1) \\ E(Y_2) \end{bmatrix} = \begin{bmatrix} 0.2 \times 95 + 0.5 \times 75 + 0.3 \times 55 \\ 0.2 \times 90 + 0.5 \times 70 + 0.3 \times 50 \end{bmatrix} = \begin{bmatrix} 73 \\ 68 \end{bmatrix}
$$

**步骤4：计算 $E(X)E(Y)^T$**

$$
E(X)E(Y)^T = \begin{bmatrix} 7.6 \\ 0.75 \end{bmatrix} \begin{bmatrix} 73 & 68 \end{bmatrix} = \begin{bmatrix} 7.6 \times 73 & 7.6 \times 68 \\ 0.75 \times 73 & 0.75 \times 68 \end{bmatrix} = \begin{bmatrix} 554.8 & 516.8 \\ 54.75 & 51 \end{bmatrix}
$$

**步骤5：计算协方差矩阵 $C_{XY}$**

$$
C_{XY} = E(XY^T) - E(X)E(Y)^T = \begin{bmatrix} 594 & 556 \\ 57.25 & 53.5 \end{bmatrix} - \begin{bmatrix} 554.8 & 516.8 \\ 54.75 & 51 \end{bmatrix} = \begin{bmatrix} 39.2 & 39.2 \\ 2.5 & 2.5 \end{bmatrix}
$$

### 白噪声

如果对于所有的$t_1≠t_2$,随机变量$X(t_1)$独立于随机变量 $X(t_2)$,那么$X(t)$被称为白噪声。否则,$X(t)$被称为有色噪声。

这里的“白”类比的是光，白光包含所有频率的可见光。就听觉上的“白噪声”而言，白噪声包含人耳可听范围内（20Hz 到 20kHz）所有频率的声音，且能量分布均匀。比如老式电视机的雪花屏/沙沙声，下雨声等

白噪声有如下基本性质：  

零均值：对所有时间$t$，期望为$0$

同方差性： 方差是恒定的，不随时间变化。 

无自相关性： 在不同时间点上的协方差为零（即没有线性关系）。 

在卡尔曼滤波中，噪声项是驱动系统动态和影响测量的核心随机变量。下面分别给出过程噪声和观测噪声的数学定义及其协方差表达式。 

**过程噪声**

过程噪声通常加在状态方程中，用于描述模型的不确定性或外部扰动。

定义$\mathbf{w}_k \in \mathbb{R}^n$ 为过程噪声向量。它是一个零均值的白噪声序列，即$E[{w}_k]=0$。

它的协方差矩阵为：

$$
\begin{aligned}
E[\mathbf{w}_k \mathbf{w}_j^T] =
\begin{cases}
\mathbf{Q}_k, & k = j \\
\mathbf{0}, & k \neq j
\end{cases}
\end{aligned}
$$

各部分含义：

$\mathbf{Q}_k$：过程噪声协方差矩阵，是一个实对称半正定矩阵。

对角线元素 $[\mathbf{Q}k]{ii}$：表示第 $i$ 个状态变量所受过程噪声的方差，反映了该状态分量随机波动的强度。

非对角线元素 $[\mathbf{Q}k]{ij}$：表示在同一时刻，第 $i$ 个状态变量与第 $j$ 个状态变量的过程噪声之间的协方差。如果非零，说明不同维度的噪声源之间存在相关性。


**观测噪声**

观测噪声加在观测方程(输出方程)中，描述传感器测量误差。

定义$\mathbf{v}_k \in \mathbb{R}^m$ 为观测噪声向量。它是一个零均值的白噪声序列，即$E[\mathbf{v}_k]=0$。

它的协方差矩阵为：  

$$
\begin{aligned}
E[\mathbf{v}_k \mathbf{v}_j^T] =
\begin{cases} 
\mathbf{R}_k, & k = j \\
0, & k \neq j
\end{cases}
\end{aligned}
$$

各部分含义：

$\mathbf{R}_k$：观测噪声协方差矩阵，是一个实对称正定矩阵（通常假设正定以保证滤波器的更新步骤稳定）。

对角线元素 $[\mathbf{R}k]{ii}$：表示第 $i$ 个传感器的测量方差，反映了该传感器的精度（数值越大，代表测量噪声越强，可信度越低）。

非对角线元素 $[\mathbf{R}k]{ij}$：表示在同一时刻，第 $i$ 个传感器与第 $j$ 个传感器的测量误差之间的协方差。如果不同传感器的误差源是独立的，则这些非对角线元素为 0，此时 $\mathbf{R}_k$ 是一个对角矩阵。


在标准卡尔曼滤波中，通常假设过程噪声与观测噪声在任何时刻都不相关：

$$
\begin{aligned}
E[\mathbf{w}_k \mathbf{v}_j^T] = \mathbf{0} \quad \text{对于所有 } k, j
\end{aligned}
$$

### 递推最小二乘估计

本节将要讨论如何基于带噪声的测量值去估计一个常量。例如,假设我们有一个电阻,但是我们不知道其电阻值是多少。我们用一个万用表多次测量它的阻值,但是因为这个万用表质量不是特别好,所以测量值是带噪声的。在这个例子里,我们可能只需要估计一个标量,但是通常情况下,我们会去估计一个常向量。

把这个例子用数学形式描述,假设$x$是一个固定的但是值是未知的$n$维向量,$y$是一个$k$维的包含噪声的测量值向量。我们如何能够获得$x$的最佳估计值$\hat{x} $?假设$y$中的每个元素都是$x$中元素的线性组合与噪声$v$的和,这样可以表示为:

$$
\begin{aligned}
y_1 &= H_{11}x_1 + \cdots + H_{1n}x_n + v_1 \\
&\vdots \\
y_k &= H_{k1}x_1 + \cdots + H_{kn}x_n + v_k
\end{aligned}
$$


线性的递推估计值可以写成如下形式:

$$
y_k=H_kx+v_k  
$$

$$
\hat{x_k}=\hat{x_{k-1}}  +K_k(y_k-H_k\hat{x_{k-1}})
$$

也就是说,我们基于上一步的估计值$\hat{x_{k-1}}$和新获得的测量$y_k$来计算$\hat{x_k}$，$K_k$是是待确定的增益矩阵。而$(y_k-H_k\hat{x_{k-1}})$被称为修正项。如果修正项为零,或者增益矩阵为零,那么估计值的大小在由$k-1$步到$k$步则不会改变。

在我们计算最佳增益矩阵$K_k$, 之前,让我们考虑一下线性递推估计器的估计误差均值。估计误差均值可以表示为：

$$
\begin{aligned}
E(\varepsilon_{x,k}) &= E(x - \hat{x}_k) = E(x - \hat{x}_{k-1} - K_k(y_k - H_k\hat{x}_{k-1}))\\
&= E(\varepsilon_{x,k-1} - K_k(H_kx + v_k - H_k\hat{x}_{k-1}))\\
&= E(\varepsilon_{x,k-1} - K_kH_k(x - \hat{x}_{k-1}) - K_kv_k) \\
&= (I - K_kH_k)E(\varepsilon_{x,k-1}) - K_kE(v_k)
\end{aligned}
$$


从式子可以看出，若$E(v_k)=0$，并且$E(\varepsilon_{x,k-1})=0$，那么$E(\varepsilon_{x,k})=0$。换句话说，如果测量值噪声$E(v_k)$对所有$k$都是零均值的，并且$x$的初始估计设置等于$x$的期望值$[\hat{x_0} =E(x)]$，那么$\hat{x}$的期望值对所有$k$而言都等于$x$。  

在卡尔曼滤波的假设中，所有噪声都是白噪声，其具有零均值的性质，因此第一个条件满足。  

对于第二个条件，若初始估计无偏，那么递推下去都是无偏的，但是实际过程中，初始估计很可能是有偏的，那么怎么满足呢？关键在于矩阵$(I - K_kH_k)$的谱半径小于1，根据矩阵的性质，递推下去，这一项就会收敛到零矩阵。（对于完全可观测的线性系统，如果噪声协方差正定、模型准确、增益按卡尔曼公式计算，那么误差递推矩阵$(I - K_kH_k)$的谱半径小于$1$，估计误差指数收敛。若系统部分不可观测，那么谱半径就等于$1$，在不可观测方向不收敛）



## 卡尔曼滤波  

> Smoothing, Filtering and Prediction: Estimating The Past, Present and Future.——Garry A. Einicke


## 相关代码

## 常见问题  

### 卡尔曼滤波中各项参数如何调整？

### 卡尔曼滤波与一般观测器的区别与联系

### 卡尔曼滤波与最小二乘法的区别与联系 

### 卡尔曼滤波的适用场合

## 参考   

最优状态估计:卡尔曼,H∞及非线性滤波(美)西蒙(Simon,D.)著;张勇刚,李宁,奔粤阳译.一北京:国防工业出版社,2013.5 ISBN 978-7-118-08808-3

阿波罗11号制导计算机中登月模块（Luminary099）原始代码的卡尔曼滤波部分：https://github.com/chrislgarry/Apollo-11/blob/master/Luminary099/KALMAN_FILTER.agc

网上相关资源  
